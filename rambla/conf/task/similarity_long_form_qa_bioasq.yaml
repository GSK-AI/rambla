defaults:
  - ../dataset@dataset_config: bioasq
  - ../utils@dataset_filterer_config: dataset_filterer
  - ../task_text_to_text_component@text_to_text_component_config: task_llm_component_qa
  - ../template@prompt_formatter_config: similarity_long_form_qa_sentence
  - ../response_formatter@response_formatter_config: negation_response_formatter
  - ../evaluator@evaluator_config: mcqa_evaluator
  - _self_

class_key: SimilarityLongFormTask

# Override any defaults here
prompt_formatter_config:
  var_map:
    question: question
    context: context
  index_field: ${index_field}

evaluator_config:
  params: 
    response_field: "evaluation_response"
    target_field: ${target_field}
    categories: ["0", "1", "null"]
    metric_names: [BucketHeadP65/confusion_matrix, "f1", "recall", "precision", "accuracy"]
    metric_kwargs:
      BucketHeadP65/confusion_matrix:
        labels: [0, 1, 2]

response_formatter_config:
  response_field_name: "evaluation_response"
  renaming_map:
    "yes": "1"
    "no": "0"
    "null": "null"

response_quality_evaluator_config:
  field_names: ["response", "question", "answer"]
  encoding: "text-davinci-003"

response_field_name: "response"